{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "In this tutorial we will see a few useful concepts:\n",
    "\n",
    "- Eager execution: operations are executed immediately as they are called from Python. This to make research and development more intuitive.\n",
    "- Model subclassing: we wil subclass tf.keras.Model and costumize our forward pass. Because of eager executing the forward pass can be written imperatively.\n",
    "\n",
    "We will need to:\n",
    "\n",
    "- Create a master agent supervisor\n",
    "- Create worker agents\n",
    "- Implement A3C\n",
    "- Train \n",
    "\n",
    "Paper: Asynchronous Methods for Deep Reinforcement Learning by Volodymyr Mnih\n",
    "\n",
    "\n",
    "# Task: CartPole\n",
    "\n",
    "\n",
    "\n",
    "Cartpole is a simple game in which the player needs to balance pole connected to a cart with un-actuated joint. The cart can move left or right. At the beginning cart position, velocity, pole angle, and velocity are randomly initialized between +/-0.05.\n",
    "\n",
    "The agent can apply a force of +1 or -1 to the cart which will move it left or right). The goals is to balance the pole acting on the cart.\n",
    "\n",
    "At every timestep in which the pole is upright the agent receive a positive reward of +1. If the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center the episode ends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-01T03:25:19.141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Starting worker 4\n",
      "Starting worker 5\n",
      "Starting worker 6\n",
      "Starting worker 7\n",
      "Saving best model to ., episode score: 12.0Saving best model to ., episode score: 16.0\n",
      "\n",
      "Saving best model to ., episode score: 19.0\n",
      "Saving best model to ., episode score: 17.0\n",
      "Saving best model to ., episode score: 22.0\n",
      "Saving best model to ., episode score: 34.0\n",
      "Saving best model to ., episode score: 39.0\n",
      "Saving best model to ., episode score: 45.0\n",
      "Saving best model to ., episode score: 60.0\n",
      "Saving best model to ., episode score: 94.0\n",
      "Saving best model to ., episode score: 96.0\n",
      "Saving best model to ., episode score: 104.0\n",
      "Saving best model to ., episode score: 111.0\n",
      "Saving best model to ., episode score: 139.0\n",
      "Saving best model to ., episode score: 149.0\n",
      "Saving best model to ., episode score: 199.0\n",
      "Saving best model to ., episode score: 211.0\n",
      "Saving best model to ., episode score: 226.0\n",
      "Saving best model to ., episode score: 315.0\n",
      "Saving best model to ., episode score: 497.0\n",
      "Saving best model to ., episode score: 562.0\n",
      "Saving best model to ., episode score: 588.0\n",
      "Saving best model to ., episode score: 784.0\n",
      "Saving best model to ., episode score: 954.0\n",
      "Saving best model to ., episode score: 1213.0\n",
      "Saving best model to ., episode score: 1267.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import threading\n",
    "import gym\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "verbose = False\n",
    "\n",
    "class ActorCriticModel(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dense1 = layers.Dense(100, activation='relu')\n",
    "        self.policy_logits = layers.Dense(action_size)\n",
    "        self.dense2 = layers.Dense(100, activation='relu')\n",
    "        self.values = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense1(inputs)\n",
    "        logits = self.policy_logits(x)\n",
    "        v1 = self.dense2(inputs)\n",
    "        values = self.values(v1)\n",
    "        return logits, values\n",
    "\n",
    "\n",
    "def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue,\n",
    "           total_loss, num_steps):\n",
    "    \"\"\"Store score  statistics.\n",
    "    \"\"\"\n",
    "    if global_ep_reward == 0:\n",
    "        global_ep_reward = episode_reward\n",
    "    else:\n",
    "        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode} | \"\n",
    "                  f\"Episode Reward: {int(episode_reward)} | \"\n",
    "                  f\"Steps: {num_steps} | \"\n",
    "                  f\"Worker: {worker_idx}\")\n",
    "    result_queue.put(global_ep_reward)\n",
    "    return global_ep_reward\n",
    "\n",
    "\n",
    "class RandomAgent(object):\n",
    "    \"\"\"Random Agent that will play the specified game\n",
    "    Arguments:\n",
    "      env_name: Name of the environment to be played\n",
    "      max_eps: Maximum number of episodes to run agent for.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name, max_eps):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.max_episodes = max_eps\n",
    "        self.global_moving_average_reward = 0\n",
    "        self.res_queue = Queue()\n",
    "\n",
    "    def run(self):\n",
    "        reward_avg = 0\n",
    "        for episode in range(self.max_episodes):\n",
    "\n",
    "            done = False\n",
    "            self.env.reset()\n",
    "            reward_sum = 0.0\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                # Sample randomly from the action space and step\n",
    "                _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
    "                steps += 1\n",
    "                reward_sum += reward\n",
    "                # Record statistics\n",
    "                self.global_moving_average_reward = record(episode,\n",
    "                                                         reward_sum,\n",
    "                                                         0,\n",
    "                                                         self.global_moving_average_reward,\n",
    "                                                         self.res_queue, 0, steps)\n",
    "\n",
    "                reward_avg += reward_sum\n",
    "\n",
    "        final_avg = reward_avg / float(self.max_episodes)\n",
    "        print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\n",
    "        return final_avg\n",
    "\n",
    "\n",
    "class MasterAgent(object):\n",
    "    def __init__(self,\n",
    "                 env_name='CartPole-v0',\n",
    "                 save_dir='.',\n",
    "                 algorithm='A3C',\n",
    "                 lr=0.001,\n",
    "                 max_eps=500):\n",
    "        self.game_name = env_name\n",
    "        self.save_dir = save_dir\n",
    "        self.algorithm = algorithm\n",
    "        self.max_eps = max_eps\n",
    "\n",
    "        env = gym.make(self.game_name)\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.opt = tf.train.AdamOptimizer(lr, use_locking=True)\n",
    "        print(self.state_size, self.action_size)\n",
    "\n",
    "        self.global_model = ActorCriticModel(\n",
    "            self.state_size, self.action_size)  # global network\n",
    "        self.global_model(\n",
    "            tf.convert_to_tensor(\n",
    "                np.random.random((1, self.state_size)), dtype=tf.float32))\n",
    "\n",
    "    def train(self):\n",
    "        if self.algorithm == 'random':\n",
    "            random_agent = RandomAgent(self.game_name, self.max_eps)\n",
    "            random_agent.run()\n",
    "            return\n",
    "\n",
    "        res_queue = Queue()\n",
    "\n",
    "        workers = [\n",
    "            Worker(\n",
    "                self.state_size,\n",
    "                self.action_size,\n",
    "                self.global_model,\n",
    "                self.opt,\n",
    "                res_queue,\n",
    "                i,\n",
    "                game_name=self.game_name,\n",
    "                save_dir=self.save_dir,\n",
    "                max_eps=self.max_eps)\n",
    "            for i in range(multiprocessing.cpu_count())\n",
    "        ]\n",
    "\n",
    "        for i, worker in enumerate(workers):\n",
    "            print(\"Starting worker {}\".format(i))\n",
    "            worker.start()\n",
    "\n",
    "        moving_average_rewards = []  # record episode reward to plot\n",
    "        while True:\n",
    "            reward = res_queue.get()\n",
    "            if reward is not None:\n",
    "                moving_average_rewards.append(reward)\n",
    "            else:\n",
    "                break\n",
    "        [w.join() for w in workers]\n",
    "\n",
    "        plt.plot(moving_average_rewards)\n",
    "        plt.ylabel('Moving avg episode reward')\n",
    "        plt.xlabel('Step')\n",
    "        plt.show()\n",
    "\n",
    "    def play(self):\n",
    "        env = gym.make(self.game_name).unwrapped\n",
    "        state = env.reset()\n",
    "        model = self.global_model\n",
    "        model_path = os.path.join(self.save_dir,\n",
    "                                  'model_{}.h5'.format(self.game_name))\n",
    "        print('Loading model {}'.format(model_path))\n",
    "        model.load_weights(model_path)\n",
    "        done = False\n",
    "        step_counter = 0\n",
    "        reward_sum = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render(mode='rgb_array')\n",
    "            policy, value = model(\n",
    "                tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "            policy = tf.nn.softmax(policy)\n",
    "            action = np.argmax(policy)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            print(\"{}. Reward: {}, action: {}\".format(\n",
    "                step_counter, reward_sum, action))\n",
    "            step_counter += 1\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def store(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "    # Set up global variables across different threads\n",
    "    global_episode = 0\n",
    "    # Moving average reward\n",
    "    global_moving_average_reward = 0\n",
    "    best_score = 0\n",
    "    save_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 global_model,\n",
    "                 opt,\n",
    "                 result_queue,\n",
    "                 idx,\n",
    "                 max_eps=500,\n",
    "                 game_name='CartPole-v0',\n",
    "                 save_dir='/tmp',\n",
    "                 update_freq=20,\n",
    "                 gamma=0.99):\n",
    "        super(Worker, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.result_queue = result_queue\n",
    "        self.global_model = global_model\n",
    "        self.opt = opt\n",
    "        self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "        self.worker_idx = idx\n",
    "        self.max_eps = max_eps\n",
    "        self.game_name = game_name\n",
    "        self.env = gym.make(self.game_name).unwrapped\n",
    "        self.save_dir = save_dir\n",
    "        self.update_freq = update_freq\n",
    "        self.gamma = gamma\n",
    "        self.ep_loss = 0.0\n",
    "\n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        mem = Memory()\n",
    "        while Worker.global_episode < self.max_eps:\n",
    "            current_state = self.env.reset()\n",
    "            mem.clear()\n",
    "            ep_reward = 0.\n",
    "            ep_steps = 0\n",
    "            self.ep_loss = 0\n",
    "\n",
    "            time_count = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                logits, _ = self.local_model(\n",
    "                    tf.convert_to_tensor(\n",
    "                        current_state[None, :], dtype=tf.float32))\n",
    "                probs = tf.nn.softmax(logits)\n",
    "\n",
    "                action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                if done:\n",
    "                    reward = -1\n",
    "                ep_reward += reward\n",
    "                mem.store(current_state, action, reward)\n",
    "\n",
    "                if time_count == self.update_freq or done:\n",
    "                    # Calculate gradient wrt to local model. We do so by tracking the\n",
    "                    # variables involved in computing the loss by using tf.GradientTape\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        total_loss = self.compute_loss(done, new_state, mem,\n",
    "                                                       self.gamma)\n",
    "                    self.ep_loss += total_loss\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(total_loss,\n",
    "                                          self.local_model.trainable_weights)\n",
    "                    # Push local gradients to global model\n",
    "                    self.opt.apply_gradients(\n",
    "                        zip(grads, self.global_model.trainable_weights))\n",
    "                    # Update local model with new weights\n",
    "                    self.local_model.set_weights(\n",
    "                        self.global_model.get_weights())\n",
    "\n",
    "                    mem.clear()\n",
    "                    time_count = 0\n",
    "\n",
    "                    if done:  # done and print information\n",
    "                        Worker.global_moving_average_reward = \\\n",
    "                          record(Worker.global_episode, ep_reward, self.worker_idx,\n",
    "                                 Worker.global_moving_average_reward, self.result_queue,\n",
    "                                 self.ep_loss, ep_steps)\n",
    "                        # We must use a lock to save our model and to print to prevent data races.\n",
    "                        if ep_reward > Worker.best_score:\n",
    "                            with Worker.save_lock:\n",
    "                                print(\"Saving best model to {}, \"\n",
    "                                      \"episode score: {}\".format(\n",
    "                                          self.save_dir, ep_reward))\n",
    "                                self.global_model.save_weights(\n",
    "                                    os.path.join(\n",
    "                                        self.save_dir,\n",
    "                                        'model_{}.h5'.format(self.game_name)))\n",
    "                                Worker.best_score = ep_reward\n",
    "                        Worker.global_episode += 1\n",
    "                ep_steps += 1\n",
    "\n",
    "                time_count += 1\n",
    "                current_state = new_state\n",
    "                total_step += 1\n",
    "        self.result_queue.put(None)\n",
    "\n",
    "    def compute_loss(self, done, new_state, memory, gamma=0.99):\n",
    "        if done:\n",
    "            reward_sum = 0.  # terminal\n",
    "        else:\n",
    "            reward_sum = self.local_model(\n",
    "                tf.convert_to_tensor(new_state[None, :],\n",
    "                                     dtype=tf.float32))[-1].numpy()[0]\n",
    "\n",
    "        # Get discounted rewards\n",
    "        discounted_rewards = []\n",
    "        for reward in memory.rewards[::-1]:  # reverse buffer r\n",
    "            reward_sum = reward + gamma * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "\n",
    "        logits, values = self.local_model(\n",
    "            tf.convert_to_tensor(np.vstack(memory.states), dtype=tf.float32))\n",
    "        # Get our advantages\n",
    "        advantage = tf.convert_to_tensor(\n",
    "            np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n",
    "        # Value loss\n",
    "        value_loss = advantage**2\n",
    "\n",
    "        # Calculate our policy loss\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=policy, logits=logits)\n",
    "\n",
    "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=memory.actions, logits=logits)\n",
    "        policy_loss *= tf.stop_gradient(advantage)\n",
    "        policy_loss -= 0.01 * entropy\n",
    "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "master = MasterAgent(max_eps=500)\n",
    "master.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T21:23:52.286315Z",
     "start_time": "2019-04-30T21:23:44.922648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./model_CartPole-v0.h5\n",
      "0. Reward: 1.0, action: 1\n",
      "1. Reward: 2.0, action: 0\n",
      "2. Reward: 3.0, action: 1\n",
      "3. Reward: 4.0, action: 0\n",
      "4. Reward: 5.0, action: 1\n",
      "5. Reward: 6.0, action: 0\n",
      "6. Reward: 7.0, action: 1\n",
      "7. Reward: 8.0, action: 0\n",
      "8. Reward: 9.0, action: 1\n",
      "9. Reward: 10.0, action: 0\n",
      "10. Reward: 11.0, action: 1\n",
      "11. Reward: 12.0, action: 0\n",
      "12. Reward: 13.0, action: 1\n",
      "13. Reward: 14.0, action: 0\n",
      "14. Reward: 15.0, action: 1\n",
      "15. Reward: 16.0, action: 0\n",
      "16. Reward: 17.0, action: 1\n",
      "17. Reward: 18.0, action: 0\n",
      "18. Reward: 19.0, action: 1\n",
      "19. Reward: 20.0, action: 0\n",
      "20. Reward: 21.0, action: 1\n",
      "21. Reward: 22.0, action: 1\n",
      "22. Reward: 23.0, action: 0\n",
      "23. Reward: 24.0, action: 1\n",
      "24. Reward: 25.0, action: 0\n",
      "25. Reward: 26.0, action: 1\n",
      "26. Reward: 27.0, action: 0\n",
      "27. Reward: 28.0, action: 1\n",
      "28. Reward: 29.0, action: 0\n",
      "29. Reward: 30.0, action: 1\n",
      "30. Reward: 31.0, action: 0\n",
      "31. Reward: 32.0, action: 1\n",
      "32. Reward: 33.0, action: 0\n",
      "33. Reward: 34.0, action: 1\n",
      "34. Reward: 35.0, action: 0\n",
      "35. Reward: 36.0, action: 0\n",
      "36. Reward: 37.0, action: 1\n",
      "37. Reward: 38.0, action: 0\n",
      "38. Reward: 39.0, action: 1\n",
      "39. Reward: 40.0, action: 0\n",
      "40. Reward: 41.0, action: 1\n",
      "41. Reward: 42.0, action: 1\n",
      "42. Reward: 43.0, action: 0\n",
      "43. Reward: 44.0, action: 0\n",
      "44. Reward: 45.0, action: 1\n",
      "45. Reward: 46.0, action: 0\n",
      "46. Reward: 47.0, action: 1\n",
      "47. Reward: 48.0, action: 0\n",
      "48. Reward: 49.0, action: 1\n",
      "49. Reward: 50.0, action: 1\n",
      "50. Reward: 51.0, action: 0\n",
      "51. Reward: 52.0, action: 1\n",
      "52. Reward: 53.0, action: 0\n",
      "53. Reward: 54.0, action: 0\n",
      "54. Reward: 55.0, action: 1\n",
      "55. Reward: 56.0, action: 0\n",
      "56. Reward: 57.0, action: 1\n",
      "57. Reward: 58.0, action: 1\n",
      "58. Reward: 59.0, action: 0\n",
      "59. Reward: 60.0, action: 1\n",
      "60. Reward: 61.0, action: 0\n",
      "61. Reward: 62.0, action: 0\n",
      "62. Reward: 63.0, action: 1\n",
      "63. Reward: 64.0, action: 0\n",
      "64. Reward: 65.0, action: 1\n",
      "65. Reward: 66.0, action: 1\n",
      "66. Reward: 67.0, action: 0\n",
      "67. Reward: 68.0, action: 0\n",
      "68. Reward: 69.0, action: 1\n",
      "69. Reward: 70.0, action: 0\n",
      "70. Reward: 71.0, action: 1\n",
      "71. Reward: 72.0, action: 1\n",
      "72. Reward: 73.0, action: 0\n",
      "73. Reward: 74.0, action: 1\n",
      "74. Reward: 75.0, action: 0\n",
      "75. Reward: 76.0, action: 0\n",
      "76. Reward: 77.0, action: 1\n",
      "77. Reward: 78.0, action: 1\n",
      "78. Reward: 79.0, action: 0\n",
      "79. Reward: 80.0, action: 0\n",
      "80. Reward: 81.0, action: 1\n",
      "81. Reward: 82.0, action: 1\n",
      "82. Reward: 83.0, action: 0\n",
      "83. Reward: 84.0, action: 0\n",
      "84. Reward: 85.0, action: 1\n",
      "85. Reward: 86.0, action: 1\n",
      "86. Reward: 87.0, action: 0\n",
      "87. Reward: 88.0, action: 0\n",
      "88. Reward: 89.0, action: 1\n",
      "89. Reward: 90.0, action: 1\n",
      "90. Reward: 91.0, action: 0\n",
      "91. Reward: 92.0, action: 0\n",
      "92. Reward: 93.0, action: 1\n",
      "93. Reward: 94.0, action: 1\n",
      "94. Reward: 95.0, action: 0\n",
      "95. Reward: 96.0, action: 1\n",
      "96. Reward: 97.0, action: 0\n",
      "97. Reward: 98.0, action: 0\n",
      "98. Reward: 99.0, action: 1\n",
      "99. Reward: 100.0, action: 0\n",
      "100. Reward: 101.0, action: 1\n",
      "101. Reward: 102.0, action: 1\n",
      "102. Reward: 103.0, action: 0\n",
      "103. Reward: 104.0, action: 1\n",
      "104. Reward: 105.0, action: 0\n",
      "105. Reward: 106.0, action: 0\n",
      "106. Reward: 107.0, action: 1\n",
      "107. Reward: 108.0, action: 1\n",
      "108. Reward: 109.0, action: 0\n",
      "109. Reward: 110.0, action: 1\n",
      "110. Reward: 111.0, action: 0\n",
      "111. Reward: 112.0, action: 0\n",
      "112. Reward: 113.0, action: 1\n",
      "113. Reward: 114.0, action: 0\n",
      "114. Reward: 115.0, action: 1\n",
      "115. Reward: 116.0, action: 1\n",
      "116. Reward: 117.0, action: 0\n",
      "117. Reward: 118.0, action: 1\n",
      "118. Reward: 119.0, action: 0\n",
      "119. Reward: 120.0, action: 1\n",
      "120. Reward: 121.0, action: 0\n",
      "121. Reward: 122.0, action: 0\n",
      "122. Reward: 123.0, action: 1\n",
      "123. Reward: 124.0, action: 1\n",
      "124. Reward: 125.0, action: 0\n",
      "125. Reward: 126.0, action: 1\n",
      "126. Reward: 127.0, action: 0\n",
      "127. Reward: 128.0, action: 0\n",
      "128. Reward: 129.0, action: 1\n",
      "129. Reward: 130.0, action: 1\n",
      "130. Reward: 131.0, action: 0\n",
      "131. Reward: 132.0, action: 0\n",
      "132. Reward: 133.0, action: 1\n",
      "133. Reward: 134.0, action: 1\n",
      "134. Reward: 135.0, action: 0\n",
      "135. Reward: 136.0, action: 1\n",
      "136. Reward: 137.0, action: 0\n",
      "137. Reward: 138.0, action: 0\n",
      "138. Reward: 139.0, action: 1\n",
      "139. Reward: 140.0, action: 1\n",
      "140. Reward: 141.0, action: 0\n",
      "141. Reward: 142.0, action: 1\n",
      "142. Reward: 143.0, action: 0\n",
      "143. Reward: 144.0, action: 1\n",
      "144. Reward: 145.0, action: 0\n",
      "145. Reward: 146.0, action: 0\n",
      "146. Reward: 147.0, action: 1\n",
      "147. Reward: 148.0, action: 1\n",
      "148. Reward: 149.0, action: 0\n",
      "149. Reward: 150.0, action: 1\n",
      "150. Reward: 151.0, action: 0\n",
      "151. Reward: 152.0, action: 1\n",
      "152. Reward: 153.0, action: 0\n",
      "153. Reward: 154.0, action: 0\n",
      "154. Reward: 155.0, action: 1\n",
      "155. Reward: 156.0, action: 1\n",
      "156. Reward: 157.0, action: 0\n",
      "157. Reward: 158.0, action: 1\n",
      "158. Reward: 159.0, action: 0\n",
      "159. Reward: 160.0, action: 1\n",
      "160. Reward: 161.0, action: 0\n",
      "161. Reward: 162.0, action: 0\n",
      "162. Reward: 163.0, action: 1\n",
      "163. Reward: 164.0, action: 1\n",
      "164. Reward: 165.0, action: 0\n",
      "165. Reward: 166.0, action: 1\n",
      "166. Reward: 167.0, action: 0\n",
      "167. Reward: 168.0, action: 1\n",
      "168. Reward: 169.0, action: 0\n",
      "169. Reward: 170.0, action: 1\n",
      "170. Reward: 171.0, action: 0\n",
      "171. Reward: 172.0, action: 0\n",
      "172. Reward: 173.0, action: 1\n",
      "173. Reward: 174.0, action: 1\n",
      "174. Reward: 175.0, action: 0\n",
      "175. Reward: 176.0, action: 1\n",
      "176. Reward: 177.0, action: 0\n",
      "177. Reward: 178.0, action: 1\n",
      "178. Reward: 179.0, action: 0\n",
      "179. Reward: 180.0, action: 1\n",
      "180. Reward: 181.0, action: 0\n",
      "181. Reward: 182.0, action: 1\n",
      "182. Reward: 183.0, action: 0\n",
      "183. Reward: 184.0, action: 1\n",
      "184. Reward: 185.0, action: 0\n",
      "185. Reward: 186.0, action: 1\n",
      "186. Reward: 187.0, action: 0\n",
      "187. Reward: 188.0, action: 1\n",
      "188. Reward: 189.0, action: 0\n",
      "189. Reward: 190.0, action: 0\n",
      "190. Reward: 191.0, action: 1\n",
      "191. Reward: 192.0, action: 1\n",
      "192. Reward: 193.0, action: 0\n",
      "193. Reward: 194.0, action: 1\n",
      "194. Reward: 195.0, action: 0\n",
      "195. Reward: 196.0, action: 1\n",
      "196. Reward: 197.0, action: 0\n",
      "197. Reward: 198.0, action: 1\n",
      "198. Reward: 199.0, action: 0\n",
      "199. Reward: 200.0, action: 1\n",
      "200. Reward: 201.0, action: 0\n",
      "201. Reward: 202.0, action: 1\n",
      "202. Reward: 203.0, action: 0\n",
      "203. Reward: 204.0, action: 1\n",
      "204. Reward: 205.0, action: 0\n",
      "205. Reward: 206.0, action: 1\n",
      "206. Reward: 207.0, action: 0\n",
      "207. Reward: 208.0, action: 1\n",
      "208. Reward: 209.0, action: 0\n",
      "209. Reward: 210.0, action: 1\n",
      "210. Reward: 211.0, action: 0\n",
      "211. Reward: 212.0, action: 1\n",
      "212. Reward: 213.0, action: 0\n",
      "213. Reward: 214.0, action: 1\n",
      "214. Reward: 215.0, action: 0\n",
      "215. Reward: 216.0, action: 1\n",
      "216. Reward: 217.0, action: 0\n",
      "217. Reward: 218.0, action: 1\n",
      "218. Reward: 219.0, action: 0\n",
      "219. Reward: 220.0, action: 1\n",
      "220. Reward: 221.0, action: 0\n",
      "221. Reward: 222.0, action: 1\n",
      "222. Reward: 223.0, action: 0\n",
      "223. Reward: 224.0, action: 1\n",
      "224. Reward: 225.0, action: 0\n",
      "225. Reward: 226.0, action: 1\n",
      "226. Reward: 227.0, action: 0\n",
      "227. Reward: 228.0, action: 1\n",
      "228. Reward: 229.0, action: 1\n",
      "229. Reward: 230.0, action: 0\n",
      "230. Reward: 231.0, action: 1\n",
      "231. Reward: 232.0, action: 0\n",
      "232. Reward: 233.0, action: 1\n",
      "233. Reward: 234.0, action: 0\n",
      "234. Reward: 235.0, action: 1\n",
      "235. Reward: 236.0, action: 0\n",
      "236. Reward: 237.0, action: 1\n",
      "237. Reward: 238.0, action: 0\n",
      "238. Reward: 239.0, action: 1\n",
      "239. Reward: 240.0, action: 0\n",
      "240. Reward: 241.0, action: 1\n",
      "241. Reward: 242.0, action: 1\n",
      "242. Reward: 243.0, action: 0\n",
      "243. Reward: 244.0, action: 1\n",
      "244. Reward: 245.0, action: 0\n",
      "245. Reward: 246.0, action: 1\n",
      "246. Reward: 247.0, action: 0\n",
      "247. Reward: 248.0, action: 1\n",
      "248. Reward: 249.0, action: 0\n",
      "249. Reward: 250.0, action: 1\n",
      "250. Reward: 251.0, action: 0\n",
      "251. Reward: 252.0, action: 1\n",
      "252. Reward: 253.0, action: 1\n",
      "253. Reward: 254.0, action: 0\n",
      "254. Reward: 255.0, action: 1\n",
      "255. Reward: 256.0, action: 0\n",
      "256. Reward: 257.0, action: 1\n",
      "257. Reward: 258.0, action: 0\n",
      "258. Reward: 259.0, action: 1\n",
      "259. Reward: 260.0, action: 0\n",
      "260. Reward: 261.0, action: 1\n",
      "261. Reward: 262.0, action: 0\n",
      "262. Reward: 263.0, action: 1\n",
      "263. Reward: 264.0, action: 1\n",
      "264. Reward: 265.0, action: 0\n",
      "265. Reward: 266.0, action: 1\n",
      "266. Reward: 267.0, action: 0\n",
      "267. Reward: 268.0, action: 1\n",
      "268. Reward: 269.0, action: 0\n",
      "269. Reward: 270.0, action: 1\n",
      "270. Reward: 271.0, action: 0\n",
      "271. Reward: 272.0, action: 1\n",
      "272. Reward: 273.0, action: 1\n",
      "273. Reward: 274.0, action: 0\n",
      "274. Reward: 275.0, action: 1\n",
      "275. Reward: 276.0, action: 0\n",
      "276. Reward: 277.0, action: 1\n",
      "277. Reward: 278.0, action: 0\n",
      "278. Reward: 279.0, action: 1\n",
      "279. Reward: 280.0, action: 1\n",
      "280. Reward: 281.0, action: 0\n",
      "281. Reward: 282.0, action: 1\n",
      "282. Reward: 283.0, action: 0\n",
      "283. Reward: 284.0, action: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284. Reward: 285.0, action: 0\n",
      "285. Reward: 286.0, action: 1\n",
      "286. Reward: 287.0, action: 0\n",
      "287. Reward: 288.0, action: 1\n",
      "288. Reward: 289.0, action: 1\n",
      "289. Reward: 290.0, action: 0\n",
      "290. Reward: 291.0, action: 1\n",
      "291. Reward: 292.0, action: 0\n",
      "292. Reward: 293.0, action: 1\n",
      "293. Reward: 294.0, action: 0\n",
      "294. Reward: 295.0, action: 1\n",
      "295. Reward: 296.0, action: 0\n"
     ]
    }
   ],
   "source": [
    "master.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
